
kernel/sched/core.c
lines 3538 schedule()
asmlinkage __visible void __sched schedule(void)
{
	struct task_struct *tsk = current;
	//현재 태스크를 sleep하고 다음 태스크를 스케줄한다. 현재 태스크가 깨어난 후 여전히 리스케줄 요청이 있는 경우 다시 루프를 돈다.
	sched_submit_work(tsk);
	do {
		preempt_disable(); // cpu선점 비활성화
		__schedule(false); // 스케쥴함수
		sched_preempt_enable_no_resched();
	} while (need_resched());
}
EXPORT_SYMBOL(schedule);

lines 3411 __schedule()
위 주석 내용 : 
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPT=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPT is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
CONFIG_PREEMPT 옵션을 사용하면서 선점이 활성화 되어있는 경우 선점이 가능하지만 옵션을 사용하지 않은 커널모드에서는 선점이 가능하지 않다. 하지만 예외가 있다.
- CONFIG_PREEMPT_VOLUNTARY 옵션 사용하몃너 cond_resched() 호출
- schedule()함수 명시적 호출 시
- 시스템콜 호출 후 유저 스페이스로 되돌아 갈때 (유저모드에서는 언제나 선점이 가능, 돌아가기 전에 선점 처리를 해야 속도가 빠름)
- 인터럽트 핸들러 처리 후 유저 스페이스로 되돌아 갈 때
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id(); // smp_processor_id(): get the current CPU ID 할당
	rq = cpu_rq(cpu); per-cpu 타입으로 rq주소를 읽는다.
	prev = rq->curr;

	schedule_debug(prev); // 스케줄 타임에 체크할  항목과 통계를 수행한다.

	if (sched_feat(HRTICK)) // 스케줄 클럭에 high-resolution 타이머를 사용한 경우 hrtick 타이머가 동작중이면 정지시킨다. (hrtimer는 기존 타이머 모델보다 더 세밀한 단위로 시간 제어, hrtimer는 나노초 단위로 시간을 관리함)
		hrtick_clear(rq);

	local_irq_disable(); // local cpu의 interrupt request을 비활성화한다.
	rcu_note_context_switch(preempt); // rcu을 정지 상태로 바꾸고 인터럽트도 비활성화 하고 여러 에러처리 한다.

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf); // rq 잠근다
	smp_mb__after_spinlock(); smp가 critical section에 각 프로세스의 동시 접근을 못하게 금지한다.

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1; /* promote REQ to ACT */
	update_rq_clock(rq);

	switch_count = &prev->nivcsw; // 기존 태스크의 context switch 횟수를 알아온다.
	if (!preempt && prev->state) { // 기존 task가 러닝 상태가 아니면서 preemption 카운터 비트 중 PREEMPT_ACTIVE가 설정되지 않은 경우 schedule() 함수가 호출되어 진입된 경우 PREEMPT_ACTIVE가 설정되지 않는다.
		if (unlikely(signal_pending_state(prev->state, prev))) {//기존 태스크의 시그널 처리가 필요한 경우 태스크 상태를 러닝 상태로 바꾼다. (task가 SIGKILL 요청을 받았거나 인터럽터블 상태의 task가 signal 처리를 요청받은 경우)
			prev->state = TASK_RUNNING; 
		} else { //signal 처리가 필요 없으면 task를 deactivation(dequeue) 하고 기존 task가 런큐에서 동작하지 않음을 표시한다. (DEQUEUE_SLEEP 플래그를 주어 task가 cfs 런큐에서 완전히 빠져나가는 것이 아니라 다시 enqueue됨을 알린다.)
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
			prev->on_rq = 0;

			if (prev->in_iowait) { // 기존 task가 iowait인 경우 
				atomic_inc(&rq->nr_iowait); // 원자적 연산은 하나의 프로세스가 제대로 완료 되기까지 interruption되지 않는 연산
				delayacct_blkio_start(); // CONFIG_TASK_DELAY_ACCT 커널 옵션을 사용하는 경우 task에 대한 각종 지연 stat을 얻을 수 있다.
			}

			/*
			 * If a worker went to sleep, notify and ask workqueue
			 * whether it wants to wake up a task to maintain
			 * concurrency.
			 */
			if (prev->flags & PF_WQ_WORKER) { // 기존 task가 워크큐 워커인 경우 슬립시킨다. 깨어난 후 to_wakeup task가 런큐에 없는 경우 enqueue 처리 한다.
				struct task_struct *to_wakeup;

				to_wakeup = wq_worker_sleeping(prev);
				if (to_wakeup)
					try_to_wake_up_local(to_wakeup, &rf);
			}
		}
		switch_count = &prev->nvcsw;  /* # voluntary context switches */
	}

	next = pick_next_task(rq, prev, &rf); // 높은 우선순위에 따라 다음 task 선택하는 함수
	clear_tsk_need_resched(prev); // 프로세스의 thread_info 구조체의 flags 필드 속 TIF_NEED_RESCHED를 지운다. 
	clear_preempt_need_resched(); // 프로세스의 thread_info 구조체의 flags 필드 속 TIF_NEED_RESCHED를 지운다. x86 아키텍쳐에서만 수행

	if (likely(prev != next)) { //  next task가 기존 task가 아닌 경우 rq의 context swicth counter를 증가시키고 현재 처리하는 task를 지정한다. 마지막으로 context 스위칭을 수행한다.
		rq->nr_switches++;
		rq->curr = next;
		++*switch_count;

		trace_sched_switch(preempt, prev, next); // context swicth 함수

		rq = context_switch(rq, prev, next, &rf); /* Also unlocks the rq: */
	} else { // RQCF_ACT_SKIP __schedule()로 다음 호출 시 시계 업데이트 건너뛰기를 요청
		//RQCF_UPDATED rq::lock이 마지막으로 고정된 이후 update_rq_clock()으로 통화가 이루어졌는지 여부를 나타내는 디버그 플래그
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);  lrq의 interrupt request을 unlock한다.
	}

	balance_callback(rq); // rq lock을 acquire 실패시 rq lock과 선점을 다른 프로세스가 가지고 있을시에 rq에서 탈취함.
}

lines 3332
static inline struct task_struct *
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those loose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely((prev->sched_class == &idle_sched_class ||
		    prev->sched_class == &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = fair_sched_class.pick_next_task(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto again;

		/* Assumes fair_sched_class->next == idle_sched_class */
		if (unlikely(!p))
			p = idle_sched_class.pick_next_task(rq, prev, rf);

		return p;
	}

again:
	for_each_class(class) {
		p = class->pick_next_task(rq, prev, rf);
		if (p) {
			if (unlikely(p == RETRY_TASK))
				goto again;
			return p;
		}
	}

	/* The idle class should always have a runnable task: */
	BUG();
}